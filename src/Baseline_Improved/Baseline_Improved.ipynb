{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from google.colab import drive\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cuml import UMAP, KMeans, DBSCAN\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "drive.mount('/content/drive')",
   "id": "3ff4c62422ec40d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clustering and visualization\n",
   "id": "c946791a44146977"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Parameters\n",
    "TFIDF_PATH = \"/content/drive/MyDrive/MSE 641 Project Data/tfidf_matrix_fast.pkl\"\n",
    "UMAP_DIM = 30\n",
    "UMAP_N_NEIGHBORS = 30\n",
    "SVD_COMPONENTS = 200\n",
    "K_RANGE = [5, 10, 20]\n",
    "DBSCAN_EPS = 1.2\n",
    "DBSCAN_MIN_SAMPLES = 15\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/MSE 641 Project Data/\"\n",
    "\n",
    "# Load & Preprocess\n",
    "X = joblib.load(TFIDF_PATH)\n",
    "svd = TruncatedSVD(n_components=SVD_COMPONENTS, random_state=42)\n",
    "X_svd = svd.fit_transform(X)\n",
    "norms = np.linalg.norm(X_svd, axis=1)\n",
    "X_svd = X_svd[norms > 0]\n",
    "X_svd, _ = np.unique(X_svd, axis=0, return_index=True)\n",
    "\n",
    "# UMAP Dimensionality Reduction\n",
    "X_gpu = cp.asarray(X_svd)\n",
    "umap = UMAP(\n",
    "  n_components=UMAP_DIM,\n",
    "  random_state=42,\n",
    "  n_neighbors=UMAP_N_NEIGHBORS,\n",
    "  min_dist=0.1\n",
    ")\n",
    "X_umap = cp.asnumpy(umap.fit_transform(X_gpu))\n",
    "\n",
    "# Project to 2D for plotting \n",
    "if UMAP_DIM > 2:\n",
    "    proj2d = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = proj2d.fit_transform(X_umap)\n",
    "else:\n",
    "    X_2d = X_umap\n",
    "\n",
    "# Save the 2D projection\n",
    "joblib.dump(X_2d, OUTPUT_DIR + \"umap_2d.pkl\")\n",
    "print(\"Saved 2D projection for visualization.\")\n",
    "\n",
    "# Subplot for all k\n",
    "fig, axes = plt.subplots(1, len(K_RANGE), figsize=(20, 6), sharex=True, sharey=True)\n",
    "if len(K_RANGE) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, K in enumerate(K_RANGE):\n",
    "  # K-Means clustering\n",
    "  kmeans = KMeans(n_clusters=K, random_state=42, n_init=\"auto\")\n",
    "  k_labels = kmeans.fit_predict(X_umap)\n",
    "\n",
    "  # Save K-Means labels\n",
    "  joblib.dump(k_labels, OUTPUT_DIR + f\"kmeans_labels_k{K}.pkl\")\n",
    "\n",
    "  # Cluster evaluation metrics\n",
    "  if len(np.unique(k_labels)) > 1:\n",
    "    sil = silhouette_score(X_umap, k_labels)\n",
    "    ch = calinski_harabasz_score(X_umap, k_labels)\n",
    "    db = davies_bouldin_score(X_umap, k_labels)\n",
    "  else:\n",
    "    sil, ch, db = np.nan, np.nan, np.nan\n",
    "\n",
    "  print(f\"\\n==== Results for k={K} ====\")\n",
    "  print(f\"Silhouette Score: {sil:.4f}\")\n",
    "  print(f\"Calinski-Harabasz Index: {ch:.1f}\")\n",
    "  print(f\"Davies-Bouldin Index: {db:.4f}\")\n",
    "  print(f\"KMeans cluster sizes: {np.bincount(k_labels)}\")\n",
    "\n",
    "  # DBSCAN inside each KMeans cluster to find noise\n",
    "  noise_indices = []\n",
    "  for cluster_id in np.unique(k_labels):\n",
    "    mask = (k_labels == cluster_id)\n",
    "    X_cluster = X_umap[mask]\n",
    "    idx_cluster = np.where(mask)[0]\n",
    "    if len(X_cluster) < DBSCAN_MIN_SAMPLES:\n",
    "        continue\n",
    "    dbscan = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES)\n",
    "    db_labels = dbscan.fit_predict(X_cluster)\n",
    "    noise_in_cluster = idx_cluster[db_labels == -1]\n",
    "    noise_indices.extend(noise_in_cluster.tolist())\n",
    "\n",
    "  print(f\"DBSCAN detected noise points: {len(noise_indices)}\")\n",
    "\n",
    "  # Save DBSCAN noise indices\n",
    "  joblib.dump(noise_indices, OUTPUT_DIR + f\"dbscan_noise_indices_k{K}.pkl\")\n",
    "\n",
    "  # Plot overlay\n",
    "  ax = axes[i]\n",
    "  # main clustering group\n",
    "  sc = ax.scatter(\n",
    "    X_2d[:, 0], X_2d[:, 1],\n",
    "    c=k_labels, cmap=\"tab20\",\n",
    "    s=10, alpha=0.75,\n",
    "    label=\"KMeans clusters\"\n",
    "  )\n",
    "  # noise points\n",
    "  if noise_indices:\n",
    "    ax.scatter(\n",
    "      X_2d[noise_indices, 0],\n",
    "      X_2d[noise_indices, 1],\n",
    "      c=\"k\", marker=\".\", s=8,\n",
    "      label=\"DBSCAN noise\", alpha=0.25\n",
    "      )\n",
    "  ax.set_title(f\"k={K}\")\n",
    "  ax.set_xlabel(\"Component 1\")\n",
    "  if i == 0:\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "  ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "  if i == len(K_RANGE) - 1:\n",
    "    ax.legend(markerscale=1.5)\n",
    "\n",
    "plt.suptitle(\"KMeans with DBSCAN Noise Overlay (all k)\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Save the combined plot\n",
    "plt.savefig(OUTPUT_DIR + \"kmeans_dbscan_subplots.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll done.\")\n"
   ],
   "id": "20eff1e91d8aa429"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "clustering result example for k = 5, 10, and 20",
   "id": "407138775414920f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "TOP_N = 10\n",
    "K_RANGE = [5, 10, 20]\n",
    "\n",
    "df = pd.read_json(\"/content/drive/MyDrive/MSE 641 Project Data/wildchat_en_cleaned.jsonl\", lines=True)\n",
    "\n",
    "for K_VAL in K_RANGE:\n",
    "    print(f\"\\n===== Top {TOP_N} Examples per Cluster for k = {K_VAL} =====\\n\")\n",
    "    # Load cluster labels and noise indices for this k\n",
    "    k_labels = joblib.load(OUTPUT_DIR + f\"kmeans_labels_k{K_VAL}.pkl\")\n",
    "    dbscan_noise_indices = joblib.load(OUTPUT_DIR + f\"dbscan_noise_indices_k{K_VAL}.pkl\")\n",
    "    df['cluster'] = k_labels\n",
    "    df['dbscan_noise'] = False\n",
    "    df.loc[dbscan_noise_indices, 'dbscan_noise'] = True\n",
    "\n",
    "    for cluster_id, group in df[df['dbscan_noise'] == False].groupby('cluster'):\n",
    "        count = len(group)\n",
    "        print(f\"\\n--- Cluster {cluster_id} ({count} samples) ---\")\n",
    "        for idx, row in enumerate(group.head(TOP_N).itertuples(), 1):\n",
    "            prompt = row.prompt\n",
    "            response = row.response\n",
    "            short_prompt = (prompt[:100] + \"...\") if len(prompt) > 100 else prompt\n",
    "            short_response = (response[:100] + \"...\") if len(response) > 100 else response\n",
    "            print(f\"{idx}. Prompt: {short_prompt}\")\n",
    "            print(f\"   Response: {short_response}\\n\")\n"
   ],
   "id": "fc57b288e0b04481"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
