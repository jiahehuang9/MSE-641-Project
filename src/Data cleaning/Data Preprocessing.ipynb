{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Read wildchat_en_cleaned data\n",
    "wildchat_en_path = \"/Users/hoyiwong/Library/CloudStorage/OneDrive-SharedLibraries-UniversityofWaterloo/Jiahe Huang - MSE 641 Project Data/wildchat_en_cleaned.jsonl\"\n",
    "wildchat_en = pd.read_json(wildchat_en_path, lines=True)"
   ],
   "id": "8515f8da7547159c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data preprocessing",
   "id": "7f4a541a960ce70f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Basic data inspection for wildchat_en dataset\n",
    "\n",
    "print(\"Total rows:\", wildchat_en.shape[0]) # Total number of rows\n",
    "\n",
    "duplicate_count = wildchat_en[\"conversation_id\"].duplicated().sum() # Count duplicate conversation_id entries\n",
    "print(\"Duplicate conversation_id count:\", duplicate_count)\n",
    "\n",
    "turn_min, turn_max = wildchat_en[\"turn\"].agg([\"min\", \"max\"]) # Show range of 'turn'\n",
    "print(f\"Turn column range: {turn_min} to {turn_max}\")\n",
    "\n",
    "true_toxic = wildchat_en[\"toxic\"].sum() # Count of rows with redacted == True\n",
    "print(\"Total True in 'toxic':\", true_toxic)\n",
    "\n",
    "true_redacted = wildchat_en[\"redacted\"].sum() # Count of rows with redacted == True\n",
    "print(\"Total True in 'redacted':\", true_redacted)\n"
   ],
   "id": "41700e034136df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove unnecessary columns\n",
    "wildchat_en_cleaned= wildchat_en.drop(columns=[\"language\", \"openai_moderation\", \"detoxify_moderation\", \"toxic\", \"redacted\"])"
   ],
   "id": "682f329c227eaaa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split conversation into prompt and response column\n",
    "\n",
    "# Create empty list for result\n",
    "rows = []\n",
    "\n",
    "for row in wildchat_en.itertuples(index=False):\n",
    "    convo = row.conversation\n",
    "    prompts = []\n",
    "    responses = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(convo) - 1:\n",
    "        user, assistant = convo[i], convo[i + 1]\n",
    "        if user[\"role\"] == \"user\" and assistant[\"role\"] == \"assistant\":\n",
    "            prompts.append(user[\"content\"])\n",
    "            responses.append(assistant[\"content\"])\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    # Combine all turns into single prompt/response thread\n",
    "    rows.append((\n",
    "        row.conversation_id,\n",
    "        row.model,\n",
    "        row.timestamp,\n",
    "        row.turn,\n",
    "        \"\\n\\n\".join(prompts),\n",
    "        \"\\n\\n\".join(responses),\n",
    "        row.toxic,\n",
    "        row.redacted,\n",
    "    ))\n",
    "\n",
    "# Convert to DataFrame\n",
    "flattened_df = pd.DataFrame(rows, columns=[\n",
    "    \"conversation_id\", \"model\", \"timestamp\", \"turn\", \"prompt\", \"response\", \"toxic\", \"redacted\",\n",
    "])"
   ],
   "id": "94ff2198db052fc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert all text to lowercase\n",
    "flattened_df[\"prompt\"] = flattened_df[\"prompt\"].str.lower()\n",
    "flattened_df[\"response\"] = flattened_df[\"response\"].str.lower()"
   ],
   "id": "3623c29663b79513"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove all punctuation and special characters\n",
    "RE_PUNCT = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "RE_MOJIBAKE = re.compile(r\"[^\\w\\s]{3,}\")\n",
    "\n",
    "def remove_encoding_garbage(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # ASCII encode-decode strip (mojibake nuke)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "    # Fast regex cleanup\n",
    "    text = RE_PUNCT.sub(\"\", text)\n",
    "    text = RE_MOJIBAKE.sub(\"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Append to prompt and response coloumns\n",
    "flattened_df[\"prompt\"] = flattened_df[\"prompt\"].apply(remove_encoding_garbage)\n",
    "flattened_df[\"response\"] = flattened_df[\"response\"].apply(remove_encoding_garbage)\n"
   ],
   "id": "621b7064976915c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ],
   "id": "491d233a60356c60"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "POS-filtered Lemmatization",
   "id": "50f107e2b1bf9451"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# Define function for POS-filtered lemmatization\n",
    "def pos_filtered_lemmatizer(text):\n",
    "    # Remove non-alphabetic characters (retain space)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", str(text))\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([\n",
    "        token.lemma_ for token in doc\n",
    "        if token.pos_ in {\"NOUN\", \"VERB\"}  # Filter out ADJ, ADV, etc.\n",
    "        and not token.is_stop\n",
    "        and token.is_alpha\n",
    "    ])\n",
    "\n",
    "# Load dataset\n",
    "flattened_df = pd.read_json(\"data/wildchat_en_cleaned.jsonl\", lines=True)\n",
    "\n",
    "# Checking progress\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "# Lemmatized 'prompt' and 'response' columns\n",
    "flattened_df[\"prompt\"] = flattened_df[\"prompt\"].progress_apply(pos_filtered_lemmatizer)\n",
    "\n",
    "# Save output\n",
    "flattened_df.to_json(\"data/wildchat_en_lemmatized.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n"
   ],
   "id": "898f67d9270483b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# Load dataset\n",
    "flattened_df = pd.read_json(\"data/wildchat_en_lemmatized.jsonl\", lines=True)\n",
    "flattened_df = flattened_df.drop(columns=[\"toxic\", \"redacted\"])\n",
    "\n",
    "# Efficient lemmatization function using spaCy pipe\n",
    "def lemmatize_full(texts, n_process=2, batch_size=300):\n",
    "    return [\n",
    "        \" \".join(token.lemma_ for token in doc if token.is_alpha)\n",
    "        for doc in tqdm(nlp.pipe(texts, batch_size=batch_size, n_process=n_process), total=len(texts), desc=\"Progress\")\n",
    "    ]\n",
    "\n",
    "# Apply to 'prompt' and 'response'\n",
    "flattened_df[\"prompt\"] = lemmatize_full(flattened_df[\"prompt\"].fillna(\"\"))\n",
    "flattened_df[\"response\"] = lemmatize_full(flattened_df[\"response\"].fillna(\"\"))\n",
    "\n",
    "# Save output\n",
    "flattened_df.to_json(\"data/wildchat_en_lemmatized.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n"
   ],
   "id": "6dda1aeaa51508e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Checking sample result\n",
    "df = pd.read_json(\"data/wildchat_en_lemmatized.jsonl\", lines=True)\n",
    "\n",
    "# Randomly sample 20 rows\n",
    "sample_df = df.sample(n=20, random_state=42)\n",
    "\n",
    "# Save to CSV\n",
    "sample_df.to_csv(\"data/sample_wildchat_en_lemmatized_sample.csv\", index=False)\n"
   ],
   "id": "da3f683b7dcb5afd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Remove stop words",
   "id": "fee143d7b715f514"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Make sure stopwords are available\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.update([\"prompt\", \"ar\", \"hi\", \"pleas\"]) # Update customize stopwords\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.Defaults.stop_words |= stop_words\n",
    "\n",
    "# Define remove stopwords function\n",
    "def remove_stopwords(texts, n_process=2, batch_size=100):\n",
    "    cleaned = []\n",
    "    for doc in tqdm(nlp.pipe(texts, n_process=n_process, batch_size=batch_size), total=len(texts), desc=\"Removing stopwords\"):\n",
    "        cleaned.append(\" \".join(token.text for token in doc if not token.is_stop and token.is_alpha))\n",
    "    return cleaned\n",
    "\n",
    "# Load dataset\n",
    "flattened_df = pd.read_json(\"data/wildchat_en_lemmatized.jsonl\", lines=True)\n",
    "\n",
    "# Apply with progress\n",
    "flattened_df[\"prompt\"] = remove_stopwords(flattened_df[\"prompt\"].fillna(\"\"))\n",
    "flattened_df[\"response\"] = remove_stopwords(flattened_df[\"response\"].fillna(\"\"))\n",
    "\n",
    "# Save cleaned data\n",
    "flattened_df.to_json(\"data/wildchat_cleaned_final.jsonl\", orient=\"records\", lines=True, force_ascii=False)\n"
   ],
   "id": "2adadc66acc4e1c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save a preview sample to CSV\n",
    "flattened_df.head(20).to_csv(\"data/wildchat_cleaned_final_sample.csv\", index=False)\n"
   ],
   "id": "ebc487714aa57cf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Phrase Detection\n",
   "id": "9b0311feff42a21d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ],
   "id": "73a998512794e46e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load data\n",
    "flattened_df = pd.read_json(\"data/wildchat_cleaned_final.jsonl\", lines=True)\n",
    "\n",
    "# Combine 'prompt' and 'response' into conversation\n",
    "flattened_df[\"conversation\"] = flattened_df[\"prompt\"].fillna(\"\") + \" \" + flattened_df[\"response\"].fillna(\"\")\n",
    "\n",
    "# Tokenize by whitespace\n",
    "tokenized_texts = [text.split() for text in flattened_df[\"conversation\"]]\n",
    "\n",
    "# Train Gensim Phrases mo\n",
    "bigram_model = Phrases(tokenized_texts, min_count=10, threshold=15)\n",
    "trigram_model = Phrases(bigram_model[tokenized_texts], min_count=10, threshold=10)\n",
    "bigram_phraser = Phraser(bigram_model)\n",
    "trigram_phraser = Phraser(trigram_model)\n",
    "\n",
    "# Apply phrasers\n",
    "flattened_df[\"phrase_tokens\"] = [\n",
    "    trigram_phraser[bigram_phraser[tokens]] for tokens in tqdm(tokenized_texts, desc=\"Applying Phrase Detection\")\n",
    "]\n",
    "\n",
    "# Join tokens back into string format\n",
    "flattened_df[\"phrase_text\"] = [\" \".join(tokens) for tokens in flattened_df[\"phrase_tokens\"]]\n",
    "\n",
    "# Save final version\n",
    "flattened_df.to_json(\"data/wildchat_phrase_detected.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
   ],
   "id": "fca89164fbe83f1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Checking\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Count detected phrases with underscores\n",
    "all_phrases = list(itertools.chain.from_iterable(flattened_df[\"phrase_tokens\"]))\n",
    "phrase_counts = Counter([tok for tok in all_phrases if \"_\" in tok])\n",
    "\n",
    "# Show top 30 phrases\n",
    "print(\"\\nTop Phrases:\")\n",
    "for phrase, count in phrase_counts.most_common(30):\n",
    "    print(f\"{phrase}: {count}\")"
   ],
   "id": "47ff3fd304f59d0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Text Vectorization",
   "id": "616a8f21672189e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# Load data\n",
    "flattened_df = pd.read_json(\"data/wildchat_phrase_detected.jsonl\", lines=True)\n",
    "conversation = flattened_df[\"phrase_text\"].tolist()\n",
    "\n",
    "# Init vectorizer \n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,        # fewer dimensions = faster\n",
    "    ngram_range=(1, 2),       # unigrams + bigrams only\n",
    "    lowercase=False,\n",
    "    max_df=0.95,\n",
    "    min_df=10,\n",
    "    norm='l2',\n",
    "    use_idf=True,\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "# Fit on full data\n",
    "print(\"Fitting vectorizer...\")\n",
    "vectorizer.fit(conversation)\n",
    "\n",
    "# Transform in chunks to reduce memory usage\n",
    "chunk_size = 10000\n",
    "X_parts = []\n",
    "\n",
    "with tqdm(total=len(conversation), desc=\"TF-IDF transforming\") as pbar:\n",
    "    for i in range(0, len(conversation), chunk_size):\n",
    "        chunk = conversation[i:i+chunk_size]\n",
    "        X_chunk = vectorizer.transform(chunk)\n",
    "        X_parts.append(X_chunk)\n",
    "        pbar.update(len(chunk))\n",
    "\n",
    "X_convo = vstack(X_parts)\n",
    "\n",
    "# Save vectorizer and matrix\n",
    "joblib.dump(vectorizer, \"data/tfidf_vectorizer_fast.pkl\")\n",
    "joblib.dump(X_convo, \"data/tfidf_matrix_fast.pkl\")\n",
    "\n",
    "print(\"Matrix shape:\", X_convo.shape)\n",
    "print(\"Vocabulary size:\", len(vectorizer.get_feature_names_out()))"
   ],
   "id": "169c386c3ed010a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get feature names and average TF-IDF score\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "mean_tfidf_scores = X_convo.mean(axis=0).A1\n",
    "\n",
    "# Check Top 50 \n",
    "top_indices = mean_tfidf_scores.argsort()[::-1][:50]\n",
    "top_features = feature_names[top_indices]\n",
    "top_scores = mean_tfidf_scores[top_indices]\n",
    "\n",
    "# Print the result\n",
    "print(\"Top 20 Most Important TF-IDF Features in conversation:\")\n",
    "for feature, score in zip(top_features, top_scores):\n",
    "    print(f\"{feature:<30} {score:.4f}\")"
   ],
   "id": "6bbacc73f6777d9d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
