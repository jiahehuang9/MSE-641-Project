{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import cupy as cp\n",
    "from google.colab import drive\n",
    "from cuml import UMAP, KMeans, DBSCAN\n",
    "import os, re, json, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import groupby\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Path\n",
    "DATA_DIR = \"/content/drive/MyDrive/MSE 641 Project Data\"\n",
    "ONET_CSV = os.path.join(DATA_DIR, \"Task_Statements_standardized.csv\")\n",
    "JSONL_CHAT = os.path.join(DATA_DIR, \"wildchat_en_cleaned.jsonl\")\n",
    "METRICS_CSV = os.path.join(DATA_DIR, \"bert_kmeans_dbscan_metrics_best_per_k.csv\")\n",
    "LABEL_GLOB = os.path.join(DATA_DIR, \"bert_kmeans_labels_k*_best.pkl\")\n",
    "OUT_CSV = os.path.join(DATA_DIR, \"onet_mapping_results.csv\")\n",
    "OUT_JSONL = os.path.join(DATA_DIR, \"onet_mapping_results.jsonl\")\n",
    "\n",
    "TOP_K_CANDIDATES = 5  # how many O*NET candidates to show to LLM\n",
    "REP_TOP_N = 5  # how many representative samples per cluster (by center similarity)\n",
    "\n",
    "# OpenAI client or fallback\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "USE_LLM = False\n",
    "try:\n",
    "  if not openai_api_key:\n",
    "    # Try Colab Secrets if available\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    openai_api_key = userdata.get(\"OPENAI_API_KEY\") or \"\"\n",
    "    if openai_api_key:\n",
    "      os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "if openai_api_key:\n",
    "  try:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "    LLM_MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "    USE_LLM = True\n",
    "    print(\"[INFO] OPENAI_API_KEY detected. LLM selection enabled.\")\n",
    "  except Exception as e:\n",
    "    print(f\"[WARN] Failed to init OpenAI client: {e}\")\n",
    "    print(\"[WARN] Proceeding WITHOUT LLM (top-1 cosine fallback).\")\n",
    "else:\n",
    "  print(\"[WARN] OPENAI_API_KEY not set. Proceeding WITHOUT LLM (top-1 cosine fallback).\")\n",
    "\n",
    "# Load O*NET and embed\n",
    "if not os.path.exists(ONET_CSV):\n",
    "    raise FileNotFoundError(f\"O*NET CSV not found: {ONET_CSV}\")\n",
    "\n",
    "onet_df = pd.read_csv(ONET_CSV)\n",
    "onet_codes = onet_df[\"O*NET-SOC Code\"].astype(str).tolist()\n",
    "onet_titles = onet_df[\"Title\"].astype(str).tolist()\n",
    "onet_descs = onet_df[\"Task\"].astype(str).tolist()\n",
    "onet_texts = onet_df[\"task_standardized\"].astype(str).tolist()\n",
    "\n",
    "print(f\"[INFO] Loaded O*NET rows: {len(onet_texts)}\")\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "onet_embs = embed_model.encode(onet_texts, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Load clustered conversations (prompt+response)\n",
    "texts = []\n",
    "with open(JSONL_CHAT, \"r\", encoding=\"utf-8\") as f:\n",
    "  for line in f:\n",
    "    if not line.strip():\n",
    "      continue\n",
    "    obj = json.loads(line)\n",
    "    if \"prompt\" in obj and \"response\" in obj:\n",
    "      p = str(obj[\"prompt\"]).strip()\n",
    "      r = str(obj[\"response\"]).strip()\n",
    "      texts.append((p + \" \" + r).strip())\n",
    "print(f\"[INFO] Loaded conversations: {len(texts)}\")\n",
    "\n",
    "# Determine k values to process\n",
    "if os.path.exists(METRICS_CSV):\n",
    "  summary_df = pd.read_csv(METRICS_CSV)\n",
    "  best_ks = sorted(pd.unique(summary_df[\"k\"]))\n",
    "else:\n",
    "  label_files = glob.glob(LABEL_GLOB)\n",
    "  best_ks = sorted({int(re.search(r\"k(\\d+)\", os.path.basename(p)).group(1)) for p in label_files})\n",
    "print(\"[INFO] k values found:\", best_ks)\n",
    "\n",
    "# Mapping\n",
    "results = []\n",
    "\n",
    "for k in tqdm(best_ks, desc=\"Mapping per k\"):\n",
    "  labels_path = os.path.join(DATA_DIR, f\"bert_kmeans_labels_k{k}_best.pkl\")\n",
    "  if not os.path.exists(labels_path):\n",
    "    print(f\"[skip] labels not found for k={k}: {labels_path}\")\n",
    "    continue\n",
    "\n",
    "  labels = joblib.load(labels_path)\n",
    "  if len(labels) != len(texts):\n",
    "    print(f\"[skip] len(labels) != len(texts) for k={k}: {len(labels)} vs {len(texts)}\")\n",
    "    continue\n",
    "\n",
    "  df = pd.DataFrame({\"text\": [t for t in texts], \"label\": labels})\n",
    "  # deterministic order\n",
    "  for cid, group in df.groupby(\"label\", sort=True):\n",
    "    all_texts = group[\"text\"].tolist()\n",
    "\n",
    "    # Representative samples: closest to cluster center in embedding space\n",
    "    if not all_texts:\n",
    "      samples = []\n",
    "    else:\n",
    "      text_embs = embed_model.encode(all_texts, batch_size=32, show_progress_bar=False)\n",
    "      center = text_embs.mean(axis=0).reshape(1, -1)\n",
    "      sims = cosine_similarity(center, text_embs)[0]\n",
    "      n_pick = min(REP_TOP_N, len(all_texts))\n",
    "      top_idxs = np.argsort(-sims)[:n_pick]\n",
    "      samples = [all_texts[i] for i in top_idxs]\n",
    "      # Deduplicate samples but keep order\n",
    "      samples = [*dict.fromkeys(samples)]\n",
    "\n",
    "    if not samples:\n",
    "      results.append({\n",
    "        \"k\": k, \"Cluster\": int(cid),\n",
    "        \"O*NET Code\": None, \"Title\": None, \"Description\": None,\n",
    "        \"Similarity\": None, \"GPT Choice\": \"None\", \"Samples\": samples\n",
    "      })\n",
    "      continue\n",
    "\n",
    "    # Top-K O*NET by cosine similarity\n",
    "    sample_vec = embed_model.encode(\" \".join(samples), convert_to_numpy=True)\n",
    "    scores = cosine_similarity(sample_vec.reshape(1, -1), onet_embs)[0]\n",
    "    cand_idxs = np.argsort(-scores)[:TOP_K_CANDIDATES]\n",
    "\n",
    "    if USE_LLM:\n",
    "      # Build prompt for LLM disambiguation\n",
    "      prompt = \"Cluster samples:\\n\"\n",
    "      for i, s in enumerate(samples, 1):\n",
    "        snippet = s.replace(\"\\n\", \" \")[:200]\n",
    "        prompt += f\"{i}. {snippet}…\\n\"\n",
    "      prompt += f\"\\nTop {TOP_K_CANDIDATES} O*NET candidates:\\n\"\n",
    "      for i, idx in enumerate(cand_idxs, 1):\n",
    "        prompt += (f\"{i}. Code: {onet_codes[idx]}, Title: {onet_titles[idx]}\\n\"\n",
    "              f\"Desc: {onet_descs[idx][:200]}…\\n\")\n",
    "      prompt += (f\"\\nWhich single candidate (1–{TOP_K_CANDIDATES}) best matches these samples? \"\n",
    "                  f\"If none, reply 'None'. Just answer the number or None.\")\n",
    "\n",
    "      try:\n",
    "        resp = client.chat.completions.create(\n",
    "          model=LLM_MODEL,\n",
    "          messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "          temperature=0.0,\n",
    "          max_tokens=10,\n",
    "        )\n",
    "        choice = (resp.choices[0].message.content or \"\").strip()\n",
    "      except Exception as e:\n",
    "        print(f\"[warn] LLM error on k={k}, cluster={cid}: {e}\")\n",
    "        choice = \"None\"\n",
    "\n",
    "      if choice.isdigit() and 1 <= int(choice) <= TOP_K_CANDIDATES:\n",
    "        sel  = cand_idxs[int(choice) - 1]\n",
    "        code, title, desc, sim = onet_codes[sel], onet_titles[sel], onet_descs[sel], float(scores[sel])\n",
    "      else:\n",
    "        code, title, desc, sim = None, None, None, None\n",
    "  else:\n",
    "    # in case LLM does not, pick top-1 cosine candidate\n",
    "    sel  = cand_idxs[0]\n",
    "    code, title, desc, sim = onet_codes[sel], onet_titles[sel], onet_descs[sel], float(scores[sel])\n",
    "    choice = \"1* (no-LLM fallback)\"\n",
    "\n",
    "  results.append({\n",
    "      \"k\": k,\n",
    "      \"Cluster\": int(cid),\n",
    "      \"O*NET Code\": code,\n",
    "      \"Title\": title,\n",
    "      \"Description\": desc,\n",
    "      \"Similarity\": sim,\n",
    "      \"GPT Choice\": choice,\n",
    "      \"Samples\": samples\n",
    "    })\n",
    "\n",
    "# print per-k details\n",
    "results_sorted = sorted(results, key=lambda x: (x[\"k\"], x[\"Cluster\"]))\n",
    "for k_val, group in groupby(results_sorted, key=lambda x: x[\"k\"]):\n",
    "  print(\"\\n\" + \"=\" * 40)\n",
    "  print(f\"Results for k = {k_val}\")\n",
    "  print(\"=\" * 40)\n",
    "  for r in group:\n",
    "    print(f\"\\nCluster {r['Cluster']} → {r['O*NET Code'] or 'None'}: {r.get('Title','') or 'No good match'}\")\n",
    "    print(f\"Desc: {r.get('Description','') or ''}\")\n",
    "    print(f\"Similarity: {r.get('Similarity','')}\")\n",
    "    print(f\"GPT Picked: {r.get('GPT Choice','')}\")\n",
    "    print(\"Samples:\")\n",
    "    for i, s in enumerate(r.get(\"Samples\", []), 1):\n",
    "        snippet = s.replace(\"\\n\", \" \")[:300]\n",
    "        print(f\"{i}. {snippet}…\")\n",
    "\n",
    "# saving (CSV + JSONL)\n",
    "def _to_py_scalar(x):\n",
    "  if isinstance(x, np.generic):\n",
    "    return x.item()\n",
    "  return x\n",
    "\n",
    "def _sanitize_val(v):\n",
    "  if isinstance(v, float) and (np.isnan(v) or np.isinf(v)):\n",
    "    return None\n",
    "  if isinstance(v, (np.floating, np.integer, np.bool_)):\n",
    "    return v.item()\n",
    "  if isinstance(v, list):\n",
    "    return [_sanitize_val(x) for x in v]\n",
    "  if isinstance(v, dict):\n",
    "    return {k: _sanitize_val(x) for k, x in v.items()}\n",
    "  return v\n",
    "\n",
    "def _json_sanitized_records(df: pd.DataFrame):\n",
    "  records = df.to_dict(orient=\"records\")\n",
    "  return [{k: _sanitize_val(v) for k, v in rec.items()} for rec in records]\n",
    "\n",
    "# Normalize records & deduplicate by (k, Cluster)\n",
    "rows  = [{k: _to_py_scalar(v) for k, v in r.items()} for r in results]\n",
    "df_all = pd.DataFrame(rows)\n",
    "dedup_keys = [c for c in [\"k\", \"Cluster\"] if c in df_all.columns]\n",
    "if dedup_keys:\n",
    "  df_all = df_all.drop_duplicates(subset=dedup_keys, keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "# Clean records for JSONL/CSV\n",
    "clean_records = _json_sanitized_records(df_all)\n",
    "\n",
    "# Write JSONL\n",
    "with open(OUT_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "  for rec in clean_records:\n",
    "    json.dump(rec, f, ensure_ascii=False, allow_nan=False)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "# Write CSV (stringify lists/dicts)\n",
    "df_csv = pd.DataFrame(clean_records).copy()\n",
    "for col in df_csv.columns:\n",
    "  if df_csv[col].apply(lambda v: isinstance(v, (list, dict))).any():\n",
    "    df_csv[col] = df_csv[col].apply(lambda v: json.dumps(v, ensure_ascii=False))\n",
    "df_csv.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"\\n[INFO] Saved:\", OUT_CSV)\n",
    "print(\"[INFO] Saved:\", OUT_JSONL)\n",
    "\n",
    "# Statistics\n",
    "def _norm_code_series(s: pd.Series) -> pd.Series:\n",
    "  s = s.astype(str).str.strip()\n",
    "  return s.replace({\"\": np.nan, \"None\": np.nan, \"none\": np.nan, \"nan\": np.nan})\n",
    "\n",
    "print(\"\\n===== STATISTICS =====\")\n",
    "df_stats = pd.DataFrame(clean_records).copy()\n",
    "\n",
    "# figure out code/title columns\n",
    "code_col_candidates = [\"O*NET Code\", \"O_NET_Code\", \"ONET Code\", \"ONET_Code\", \"Code\"]\n",
    "code_col = next((c for c in code_col_candidates if c in df_stats.columns), None)\n",
    "if code_col is None:\n",
    "  raise KeyError(f\"Cannot find O*NET code column in: {df_stats.columns.tolist()}\")\n",
    "title_col = next((c for c in [\"Title\", \"ONET Title\", \"O*NET Title\"] if c in df_stats.columns), None)\n",
    "\n",
    "df_stats[code_col] = _norm_code_series(df_stats[code_col])\n",
    "\n",
    "total_rows = len(df_stats)\n",
    "mapped_mask = df_stats[code_col].notna()\n",
    "mapped_rows = int(mapped_mask.sum())\n",
    "overall_rate = (mapped_rows / total_rows * 100) if total_rows else np.nan\n",
    "\n",
    "print(f\"Total clusters (rows): {total_rows}\")\n",
    "print(f\"Mapped clusters:       {mapped_rows} ({overall_rate:.2f}%)\")\n",
    "\n",
    "mapped_df = df_stats.loc[mapped_mask].copy()\n",
    "if title_col:\n",
    "  mapped_df[\"Category\"] = mapped_df[code_col].astype(str) + \" — \" + mapped_df[title_col].astype(str)\n",
    "else:\n",
    "  mapped_df[\"Category\"] = mapped_df[code_col].astype(str)\n",
    "\n",
    "counts = mapped_df[\"Category\"].value_counts(dropna=False)\n",
    "percentages = (counts / mapped_rows * 100).round(2) if mapped_rows else counts * np.nan\n",
    "category_summary = pd.DataFrame({\n",
    "  \"Category\": counts.index,\n",
    "  \"Count\": counts.values,\n",
    "  \"Percentage_of_Mapped(%)\": percentages.values\n",
    "})\n",
    "\n",
    "print(\"\\nCategory distribution (among mapped clusters):\")\n",
    "print(category_summary.to_string(index=False))\n",
    "\n",
    "if \"k\" in df_stats.columns:\n",
    "  by_k = (\n",
    "    df_stats.assign(mapped=mapped_mask.astype(int))\n",
    "      .groupby(\"k\")[\"mapped\"]\n",
    "      .agg(total=\"count\", mapped=\"sum\")\n",
    "      .reset_index()\n",
    ")\n",
    "  by_k[\"mapping_rate_%\"] = (by_k[\"mapped\"] / by_k[\"total\"] * 100).round(2)\n",
    "  print(\"\\nMapping rate by k:\")\n",
    "  print(by_k.to_string(index=False))\n",
    "else:\n",
    "  print(\"\\nNo 'k' column found; skip per-k mapping rate.\")\n",
    "\n",
    "# display top 10 results\n",
    "TOP_N = 10\n",
    "print(f\"\\nTop {min(TOP_N, len(category_summary))} categories by count:\")\n",
    "print(category_summary.head(TOP_N).to_string(index=False))\n"
   ],
   "id": "8ce7dd430158e65c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
