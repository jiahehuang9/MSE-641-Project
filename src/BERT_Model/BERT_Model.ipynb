{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from google.colab import drive\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from cuml import UMAP, KMeans, DBSCAN\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clustering and visualization",
   "id": "4b5e6832986f6765"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load data\n",
    "in_path = \"/content/drive/MyDrive/MSE 641 Project Data/wildchat_en_cleaned.jsonl\"\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "  for line in f:\n",
    "    obj = json.loads(line)\n",
    "    if \"prompt\" in obj and \"response\" in obj:\n",
    "      prompts.append(obj[\"prompt\"])\n",
    "      responses.append(obj[\"response\"])\n",
    "texts = [p + \" \" + r for p, r in zip(prompts, responses)]\n",
    "\n",
    "# BERT embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(texts, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# SVD first to 100\n",
    "SVD_DIM = 100\n",
    "svd = TruncatedSVD(n_components=SVD_DIM, random_state=42)\n",
    "X_svd = svd.fit_transform(embeddings)\n",
    "\n",
    "# UMAP to 30\n",
    "umap_neighbors = [30]\n",
    "umap_dims = [30]\n",
    "k_range = [5, 10, 20]\n",
    "dbscan_eps = 1.2\n",
    "dbscan_min_samples = 15\n",
    "\n",
    "results = []\n",
    "best_k_results = {}\n",
    "\n",
    "for umap_dim in umap_dims:\n",
    "  for umap_nn in umap_neighbors:\n",
    "    umap_model = umap.UMAP(n_components=umap_dim, n_neighbors=umap_nn, random_state=42)\n",
    "    X_umap = umap_model.fit_transform(X_svd)\n",
    "\n",
    "    for k in k_range:\n",
    "      kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "      kmeans_labels = kmeans.fit_predict(X_umap)\n",
    "\n",
    "      final_labels = np.full(len(X_umap), -1)\n",
    "      for i in range(k):\n",
    "        idx = (kmeans_labels == i)\n",
    "        if np.sum(idx) == 0:\n",
    "            continue\n",
    "        subcluster = X_umap[idx]\n",
    "        dbscan = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples)\n",
    "        sub_labels = dbscan.fit_predict(subcluster)\n",
    "        for j, sub_label in enumerate(sub_labels):\n",
    "            global_idx = np.where(idx)[0][j]\n",
    "            if sub_label >= 0:\n",
    "                final_labels[global_idx] = i * 1000 + sub_label\n",
    "            else:\n",
    "                final_labels[global_idx] = -1\n",
    "\n",
    "      valid = final_labels >= 0\n",
    "      X_valid = X_umap[valid]\n",
    "      labels_valid = final_labels[valid]\n",
    "      if len(np.unique(labels_valid)) > 1:\n",
    "        sil = silhouette_score(X_valid, labels_valid)\n",
    "        ch = calinski_harabasz_score(X_valid, labels_valid)\n",
    "        db = davies_bouldin_score(X_valid, labels_valid)\n",
    "      else:\n",
    "        sil, ch, db = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "      results.append({\n",
    "        'umap_dim': umap_dim,\n",
    "        'umap_nn': umap_nn,\n",
    "        'k': k,\n",
    "        'dbscan_eps': dbscan_eps,\n",
    "        'dbscan_min_samples': dbscan_min_samples,\n",
    "        'silhouette': sil,\n",
    "        'calinski_harabasz': ch,\n",
    "        'davies_bouldin': db,\n",
    "        'n_clusters': len(np.unique(labels_valid)),\n",
    "      })\n",
    "\n",
    "      if k not in best_k_results or sil > best_k_results[k][\"silhouette\"]:\n",
    "        best_k_results[k] = {\n",
    "          \"silhouette\": sil,\n",
    "          \"dim\": umap_dim,\n",
    "          \"nn\": umap_nn,\n",
    "          \"k\": k,\n",
    "          \"labels\": final_labels,\n",
    "          \"X_umap\": X_umap\n",
    "          }\n",
    "\n",
    "# Save and Plot for Best Per-k Results\n",
    "for k, result in best_k_results.items():\n",
    "  X_umap = result[\"X_umap\"]\n",
    "  labels = result[\"labels\"]\n",
    "  dim = result[\"dim\"]\n",
    "  nn = result[\"nn\"]\n",
    "\n",
    "  joblib.dump(X_umap, f\"/content/drive/MyDrive/MSE 641 Project Data/bert_umap_matrix_k{k}_best.pkl\")\n",
    "  joblib.dump(labels, f\"/content/drive/MyDrive/MSE 641 Project Data/bert_kmeans_labels_k{k}_best.pkl\")\n",
    "\n",
    "  # visualization\n",
    "  if X_umap.shape[1] > 2:\n",
    "    svd2d = TruncatedSVD(n_components=2, random_state=42)\n",
    "    X_2d = svd2d.fit_transform(X_umap)\n",
    "  else:\n",
    "    X_2d = X_umap\n",
    "\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap='tab20', s=10, alpha=0.7)\n",
    "  plt.title(f\"BERT + SVD({SVD_DIM}) + UMAP({dim},{nn}) + KMeans(k={k}) + DBSCAN(1.2,15)\")\n",
    "  plt.xlabel(\"Component 1\")\n",
    "  plt.ylabel(\"Component 2\")\n",
    "  plt.grid(True, linestyle='--', alpha=0.3)\n",
    "  plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  plt.savefig(f\"/content/drive/MyDrive/MSE 641 Project Data/bert_kmeans_best_k{k}_plot.png\")\n",
    "  plt.close()\n",
    "\n",
    "# Save evaluation summary\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_path = \"/content/drive/MyDrive/MSE 641 Project Data/bert_kmeans_dbscan_metrics_best_per_k.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n"
   ],
   "id": "93a02520bb3f9770"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clustering result example",
   "id": "9fea551c006c934b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute & display metrics from previously saved files\n",
    "import os, re, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "OUT_DIR = \"/content/drive/MyDrive/MSE 641 Project Data\"\n",
    "\n",
    "label_files = sorted(glob.glob(os.path.join(OUT_DIR, \"bert_kmeans_labels_k*_best.pkl\")))\n",
    "if not label_files:\n",
    "  raise FileNotFoundError(f\"No saved label files like 'bert_kmeans_labels_k*_best.pkl' found in {OUT_DIR}\")\n",
    "\n",
    "rows = []\n",
    "for lf in label_files:\n",
    "  fname = os.path.basename(lf)\n",
    "  m = re.search(r\"k(\\d+)\", fname)\n",
    "  if not m:\n",
    "    print(f\"[skip] cannot parse k from: {fname}\")\n",
    "    continue\n",
    "  k = int(m.group(1))\n",
    "\n",
    "  umap_path = os.path.join(OUT_DIR, f\"bert_umap_matrix_k{k}_best.pkl\")\n",
    "  if not os.path.exists(umap_path):\n",
    "    print(f\"[skip] UMAP matrix for k={k} not found: {umap_path}\")\n",
    "    continue\n",
    "\n",
    "  # load data\n",
    "  X_umap = joblib.load(umap_path)\n",
    "  labels = joblib.load(lf)\n",
    "\n",
    "  # calculate evaluation metrics\n",
    "  mask = labels >= 0\n",
    "  X_valid = X_umap[mask]\n",
    "  labels_valid = labels[mask]\n",
    "  if X_valid.shape[0] > 0 and np.unique(labels_valid).size > 1:\n",
    "    sil = silhouette_score(X_valid, labels_valid)\n",
    "    ch = calinski_harabasz_score(X_valid, labels_valid)\n",
    "    db = davies_bouldin_score(X_valid, labels_valid)\n",
    "  else:\n",
    "    sil = ch = db = np.nan\n",
    "\n",
    "  print(f\"\\n==== Metrics for k={k} (valid points: {mask.sum()}/{len(labels)}) ====\")\n",
    "  print(f\"Silhouette Score: {sil:.4f}\" if np.isfinite(sil) else \"Silhouette Score: NaN\")\n",
    "  print(f\"Calinski-Harabasz Index: {ch:.1f}\"  if np.isfinite(ch)  else \"Calinski-Harabasz Index: NaN\")\n",
    "  print(f\"Davies-Bouldin Index: {db:.4f}\" if np.isfinite(db) else \"Davies-Bouldin Index: NaN\")\n",
    "\n",
    "  rows.append({\n",
    "    \"k\": k,\n",
    "    \"silhouette\": sil,\n",
    "    \"calinski_harabasz\": ch,\n",
    "    \"davies_bouldin\": db,\n",
    "    \"valid_n\": int(mask.sum())\n",
    "  })\n",
    "\n",
    "# display\n",
    "if rows:\n",
    "  df = pd.DataFrame(rows).sort_values(\"k\")\n",
    "  print(\"\\nSummary:\")\n",
    "  print(df.to_string(index=False))\n"
   ],
   "id": "be988e89d155dc12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "clustering result example",
   "id": "1937141c5e8f9f15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# BERT clustering result\n",
    "import pandas as pd\n",
    "df_clu = pd.DataFrame({\n",
    "  'prompt': prompts,\n",
    "  'response': responses,\n",
    "  'cluster': labels\n",
    "})\n",
    "\n",
    "TOP_N = 10\n",
    "\n",
    "for clu_id, group in df_clu[df_clu['cluster'] != -1].groupby('cluster'):\n",
    "  print(f\"\\n--- Cluster {clu_id} ({len(group)} samples) ---\")\n",
    "  for idx, row in enumerate(group.head(TOP_N).itertuples(), 1):\n",
    "    short_prompt = (row.prompt[:100] + \"...\") if len(row.prompt) > 100 else row.prompt\n",
    "    short_response = (row.response[:100] + \"...\") if len(row.response) > 100 else row.response\n",
    "    print(f\"{idx}. Prompt: {short_prompt}\")\n",
    "    print(f\"   Response: {short_response}\\n\")\n",
    "\n",
    "# noise points\n",
    "noise = df_clu[df_clu['cluster'] == -1]\n",
    "if not noise.empty:\n",
    "  print(f\"\\n--- Noise (DBSCAN label=-1), {len(noise)} samples ---\")\n",
    "  for idx, row in enumerate(noise.head(TOP_N).itertuples(), 1):\n",
    "    short_prompt = (row.prompt[:100] + \"...\") if len(row.prompt) > 100 else row.prompt\n",
    "    short_response = (row.response[:100] + \"...\") if len(row.response) > 100 else row.response\n",
    "    print(f\"{idx}. Prompt: {short_prompt}\")\n",
    "    print(f\"Response: {short_response}\\n\")\n"
   ],
   "id": "a3b7109ac4e7b2bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
