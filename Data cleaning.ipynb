{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-18T20:02:54.565168Z",
     "start_time": "2025-06-18T20:02:50.250592Z"
    }
   },
   "source": [
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from nltk.stem import SnowballStemmer\n",
    "import openpyxl"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jiahe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "236f417516241895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:12:51.902022Z",
     "start_time": "2025-06-18T20:11:20.852623Z"
    }
   },
   "source": [
    "ds_cleaned = load_dataset(\"json\", data_files=\"wildchat_en_cleaned.jsonl\", split=\"train\")\n",
    "\n",
    "def extract_prompt_response_pairs(conversation):\n",
    "    \"\"\"\n",
    "    Yields (user_prompt, ai_response) pairs.\n",
    "    Each pair consists of a user's message and the *next* assistant response.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    i = 0\n",
    "    while i < len(conversation) - 1:\n",
    "        user = conversation[i]\n",
    "        assistant = conversation[i + 1]\n",
    "        if user.get('role') == 'user' and assistant.get('role') == 'assistant':\n",
    "            pairs.append({\n",
    "                \"prompt\": user.get(\"content\", \"\"),\n",
    "                \"response\": assistant.get(\"content\", \"\"),\n",
    "                \"prompt_clean\": user.get(\"content_clean\", \"\"),       \n",
    "                \"response_clean\": assistant.get(\"content_clean\", \"\") \n",
    "            })\n",
    "        i += 1\n",
    "    return pairs\n",
    "\n",
    "all_pairs = []\n",
    "for ex in ds_cleaned:\n",
    "    for pair in extract_prompt_response_pairs(ex[\"conversation\"]):\n",
    "        pair[\"conversation_id\"] = ex.get(\"conversation_id\")\n",
    "        pair[\"model\"] = ex.get(\"model\")\n",
    "        all_pairs.append(pair)\n",
    "\n",
    "with open(\"wildchat_prompt_response_pairs.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for pair in all_pairs:\n",
    "        f.write(json.dumps(pair, ensure_ascii=False) + \"\\n\")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T18:42:37.997519Z",
     "start_time": "2025-07-25T18:41:26.956244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Load Excel and clean column names\n",
    "src_file = \"Task Statements.xlsx\"\n",
    "df = pd.read_excel(src_file)\n",
    "df.rename(columns=lambda c: c.strip(), inplace=True)\n",
    "\n",
    "# Set relevant columns\n",
    "task_col = \"Task\"\n",
    "title_col = \"Title\"\n",
    "print(\"Task column:\", task_col, \"example ->\", df[task_col].iloc[0])\n",
    "print(\"Title column:\", title_col, \"example ->\", df[title_col].iloc[0])\n",
    "\n",
    "# NLP Preparation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKD\", str(text))\n",
    "    text = \"\".join(c for c in text if not unicodedata.combining(c))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+|\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \"\", text)\n",
    "    text = re.sub(r\"[.!?]\", \" \", text)\n",
    "    text = re.sub(r\"\\b(\\d+)\\b\", r\"NUM_\\1\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def standardize(text: str) -> str:\n",
    "    doc = nlp(normalize(text))\n",
    "    tokens = [\n",
    "        tok.lemma_.lower()\n",
    "        for tok in doc\n",
    "        if not tok.is_stop and not tok.is_punct and tok.lemma_.strip()\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply standardization\n",
    "df[\"task_standardized\"] = df[task_col].astype(str).apply(standardize)\n",
    "\n",
    "# Select columns to save\n",
    "out_cols = [\"O*NET-SOC Code\", title_col, task_col, \"task_standardized\"]\n",
    "df[out_cols].to_csv(\"Task_Statements_standardized.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved to Task_Statements_standardized.csv  (total {len(df)} rows)\")\n"
   ],
   "id": "69901e143efc562f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task column: Task example -> Direct or coordinate an organization's financial or budget activities to fund operations, maximize investments, or increase efficiency.\n",
      "Title column: Title example -> Chief Executives\n",
      "Saved to Task_Statements_standardized.csv  (total 18796 rows)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a44b00c2e792338"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (genai)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
